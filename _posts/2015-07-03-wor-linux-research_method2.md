---
layout: post
title: "整理自为知笔记的做Rearch和实习的一些日常笔记(2)"
date: 2015-06-15
description: "Common Sense、Internship Experience"
tag: Bundle
---

### 1. FFT

对于fft, fftshift等常规操作的理解:

![fft1](https://github.com/lmrshare/lmrshare.github.io/blob/master/images/posts/wiznote/fft1.jpg)

从右侧看长的一样，但是从左侧看，根据视觉的判断显然知道左侧的数据不一样（经过shift后是一样的），并且由于傅里叶变换是一对一的变换，所以可以知道右侧数据是不同的。
同理如下图:

基于傅里叶变换的线性特点，熟练使用下面的操作:

    fft(A) = fft(I*A) = I * fft(A) = fft(I)*A

![fft2](https://github.com/lmrshare/lmrshare.github.io/blob/master/images/posts/wiznote/fft2.jpg)

#### 数字信号处理&信号与系统[common sense]

延迟:

>* 在数字信号处理的硬件设备中，延迟实际上是由一系列的移位寄存器来实现的

互相关: Rxy(m)，x信号向右移位m后与x信号相乘再求和(等价于y信号左移然后相乘再相加), 自相关函数Rxx(m)定义与此类似, 这是一个特别的互相关.

>* 互相关函数Rxy(m)衡量两个信号x，y的相似性，自相关函数Rxx(m)衡量信号x与其时移后的信号的相似性；

物理意义:

相关函数只是反映了两个信号的相似性，与系统无关；卷积反映了线性时不变系统输入，输出和单位冲激响应的关系，是与系统有关的

>* 1.实信号的自相关为实偶函数，如果为复信号，则r(-m)与r(m)是共轭的
>* 2.不移位时，自相关最大
>* 3.对于能量信号，如果移位太大了，那么自相关会变小，当移位趋向无穷大时，自相关为0
>* 4.Rxy(m)不是偶函数，，但是有Rxy(m)=Ryx(-m)
>* 5互相关函数绝对值|Rxy(m)|小于等于两信号能量乘积的开方:|Rxy(m)|<=sqrt(Rx(0)Ry(0))=sqrt(ExEy)
>* 6对于两个能量信号，如果移位太大了，那么互相关会变小，当移位趋向无穷大时，互相关为0
>* 7.经验：噪声的自相关函数Ru(m)主要在m=0出有值，当|m|>0时，应衰减的很快，信号与噪声的互相关函数应该很小，建模时可考虑将两者的影响忽略

自相关可用于：噪声中信号的检测，信号中隐含周期性的检测，信号相关性的检测，信号时延长度的检测.

卷积:

>* 理解1：二维理解法(信号移位后求加权和)，注意:x(n)*y(n),n做横轴，m做纵轴；或者说这叫做一n的方式来思考信号
>* 理解2：x(n)*y(n)就是将x信号翻转，右移n个单位，然后与y信号相乘再相加（求和）；注意，这种理解方式与互相关统一起来了

相关vs卷积: 

>* 1.统一的理解方式中，函数的字变量只是个移动量，或者说不是两个信号的自变量；而卷积的二维理解方式中，函数的自变量确是主体，是作为后面一个信号的自变量的，也就是上面的横轴
>* 2.统一理解方式：互相关：第一个信号右移，相乘相加；卷积：第一个信号翻转右移，相乘相加
>* 3.卷积的二维理解方式：x(n)*y(n)中，n作为x信号的自变量，然后右移m个单位(m有无穷多个，也就说有无穷多个x移位信号)，y(m)作为这些信号的权值，将这些信号加权求和后得到最终的卷积信号

功率:

>* 可能是实际物理上的功率，或者信号数值的平方；

时域功率:

>* 描述信号或者时间序列f(t)的能量如何随着时间分布，数学定义为为f(t)的幅度(模)的平方

谱密度（能量谱密度）:

>* 描述信号或者时间序列f(t)的能量如何随着频率分布，或者可以叫谱密度为频域上的功率(与时域功率联系理解)；数学上的定义就为f(t)的傅立叶变换的幅度(模)的平方

功率谱密度:

>* 描述信号或者时间序列的功率是如何随着频率分布的，可叫做信号f(t)的时域功率p(t)的傅立叶变换

>* 如果信号可以看作是一个平稳随机过程，那么信号的功率谱就是信号的自相关函数的傅立叶变换；根据该属性，有关功率谱的方法均可应用于自相关

### 2. 实验笔记

k_up,k_down，分别是取得的部分编码线，两个采样有重叠，在单独进行ifft2的时候得到两张空间域图像，两张图像都会出现
模糊，但是当把两张空间域图像相加的，然后再归一化显示，得到的效果与ifft2下的kt（：，：，1）是一样的.

#### 循环卷积与线性卷积

分析时域离散线性时不变系统或者进行滤波操作的时候，需要计算两个序列的线性卷积。然而DFT只能直接计算循环卷积，如果想使用DFT计算线性卷积，就要研究线性卷积和循环卷积之间的关系。循环卷积与线性卷积的特点:

>* 1. 循环卷积：有移位，有补充，有延拓(因此是周期的)；
>* 2. 线性卷积：有移位，无补充，无延拓；

#### overlap-add与overlap-save:

这两个算法都是计算长序列卷积的，思想都是分段卷积，overlap-add进行分段卷积时用的是线性卷积，overlap-save进行分段卷积时用的是循环卷积。
设计这两个算法的时候，只需要考虑：

>* 1.原始长序列进行卷积的情况
>* 2.两种分段卷积算法的情况。
>* 3. overlap-add算法分段卷积时每段卷积结果的前半段(M-1)会出现缺失的情况[第一段除外]；
>* 4. overlap-save算法分段卷积时每段卷积结果的前半段会出现混叠的情况；

#### MRI循环卷积

>* 1.磁共振成像的混叠现象可以用循环卷积的特点来解释
>* 2.线性卷积可以用循环卷积来求解[可分为时域方法和频域方法]；具体的方法为：在xn后面补充相应数量的0来构造周期为N+M-1的循环卷积，这样一般卷积就是循环卷积的一个周期的值

原理：用0来撑开xn，以避免循环卷积带来的混叠

### 定点计算

将浮点算法转化为定点算法，首先就要把所有的小数经定标转化为整数。比如浮点算法中有一个浮点数组为`[ 0.25  0.5  1.125   5.75  7.0 ]`，用二进制表示为
`[ 0.01  0.1  1.001  101.11  111.0 ]`。将数组各分量左移三位，得`[ 10  100  1001  101110  111000 ]`，十进制表示为`[ 2  4  9  46  56 ]`。小数点位置在第3位，此数组
中的各分量都是Q3格式，这就是定标的过程。

注意:十进制小数和二进制小数并不是一一对应的，也就是有些十进制小数没有与其对应的二进制小数换种说法就是，二进制小数是十进制小数的离散点.

### 语音信号处理

#### 基本概念以及历史

>* 音高：人耳对声音调子高低的主观感受；客观上音高大小主要取决于声波基频的高低，频率高则音调高.
>* 音色：一切物体发声的原理都是振动出声由空气传出，那在物体振动的时候出来的频率都是一个波形，这个波总可以分解为一系列不同频率正弦波的叠加，含一个基波和许多谐波，即内含各种频率成分，音色的不同，就是这些谐波的含量都不相同.
>* 声音的频率：声波每秒的振动次数称为频率，如图1，频率在20hz~20khz之间称为声波；频率大于20khz称为超声波；频率小于20hz称为次声波。超声波和次声波人耳是听不到的，地震波和海啸都是次声波。有些动物的耳朵比人类要灵敏得多，比如蝙蝠就能"听到"超声波。在音频信号中，通常又把300hz到3400hz的频率范围成为“语音信号”，对应了人的发声频率。高频和低频是相对的，在语音范围中，通常把1000 hz以上的区域称为高频区，500 hz -1000 hz的区域称为中频区，低于500 hz的区域称为低频区。电话信号只覆盖了人的发生频率范围，从200hz到3400hz，AM广播达到7000hz，FM广播达到15khz，CD音质最好，覆盖了全部人耳能够听到的频率范围。

语音信号的数字表示可以分为两类： __波形表示__ ， __参数表示__ .

>* 波形表示：仅通过采样和量化保存模拟语音信号的波形
>* 参数表示：将语音信号表示为某种语音产生模型的输出，是对数字化语音进行分析和处理后得到的。

声音的编码:

>* 波形编码：波形编码是在时域上进行处理，对模拟语音按一定的速率抽样，然后将幅度样本分层量化。波形编码的基本过程可以概述为：采样——量化——编码。为了用有限的bit数表示所有的采样值，需要对采样后的离散信号进行量化处理，量化可以分为均匀量化和非均匀量化。量化完成后输出的数据就是常说的PCM数据了。在最早的电话系统中，对语音信号8khz采样，每秒产生8000个样点，对每个样值8bit编码，那么一路话音数据的码率就是8000*8=64kbps（码率就是一秒内总的bit数，码率也叫比特率）

>* 参数编码：参数编码是利用语音信息产生的数学模型，提取语音信号的特征参量，并按照模型参数重构音频信号。从原理上讲，LPC是通过分析话音波形来产生声道激励和转移函数的参数，对声音波形的编码实际就转化为对这些参数的编码，这就使声音的数据量大大减少。在接收端使用LPC分析得到的参数，通过话音合成器重构话音。合成器实际上是一个离散的随时间变化的时变线性滤波器，它代表人的话音生成系统模型。时变线性滤波器既当作预测器使用，又当作合成器使用。分析话音波形时，主要是当作预测器使用，合成话音时当作话音生成模型使用。随着话音波形的变化，周期性地使模型的参数和激励条件适合新的要求。它只能收敛到模型约束的最好质量上，力图使重建语音信号具有尽可能高的可懂性，而重建信号的波形与原始语音信号的波形相比可能会有相当大的差别。这种编码技术的优点是压缩比高，但重建音频信号的质量较差，自然度低，适用于窄带信道的语音通讯，如军事通讯、航空通讯等。美国的军方标准LPC（线性预测编码）-10，就是从语音信号中提取出来反射系数、增益、基音周期、清/浊音标志等参数进行编码的。
>* 混合编码：将上述两种编码方法结合起来，采用混合编码的方法，可以在较低的数码率上得到较高的音质。它的基本原理是合成分析法，将综合滤波器引入编码器，与分析器相结合，在编码器中将激励输入综合滤波器产生与译码器端完全一致的合成语音，然后将合成语音与原始语音相比较（波形编码思想），根据均方误差最小原则，求得最佳的激励信号，然后把激励信号以及分析出来的综合滤波器编码送给解码端。这种得到综合滤波器和最佳激励的过程称为分析（得到语音参数）；用激励和综合滤波器合成语音的过程称为综合；由此我们可以看出CELP编码把参数编码和波形编码的优点结合在了一起，使得用较低码率产生较好的音质成为可能。通过设计不同的码本和码本搜索技术，产生了很多编码标准，目前我们通讯中用到的大多数语音编码器都采用了混合编码技术。例如在互联网上的G.723.1和G.729标准，在GSM上的EFR、HR标准，在3GPP2上的EVRC、QCELP标准，在3GPP上的AMR-NB/WB标准等等。

名词解释：

>* VBR：Variable Bitrate（动态比特率）：没有固定的比特率，压缩软件在压缩时根据音频数据即时确定比特率，特点：以质量为前提兼顾文件大小。
>* CBR：constant Bitrate（常数比特率）：指文件从头到尾都是一种位速率，它压缩出来的文件体积很大，并且音质相对VBR，ABR并没有显著提升。
>* ABR：Average Bitrate（平均比特率）：是 VBR的一种插值参数。LAME针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR在指定的文件大小内，以每50帧 （30帧约1秒）为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量，可以做为VBR和CBR的一种折衷选择。
>* 采样率：单位时间内对音频信号进行采样的次数.它以赫兹(HZ)或千赫兹(KHZ)为单位.通常来说,采样率越高,单位时间内对声音采样的次数就 越多,这样音质就越好；MP3音乐的采样率一般是44.1KHZ,即每秒要对声音进行44100次分析。

音乐格式五花八门，多如牛毛，但不外乎分为两大类：

>* 音乐指令文件（如MIDI），一般由音乐创作软件制作而成，它实质上是一种音乐演奏的命令，不包括具体的声音数据，故文件很小
>* 声音文件，是通过录音设备录制的原始声音，其实质上是一种二进制的采样数据，故文件较大。

从播放形式上，声音文件还可以分为: __音频流__ 和 __非音频流__. 前者能够一边下载一边收听，比如“.WMA”、“.RA”、“.MOV”等，后者则不能。所谓流媒体技术就是把连续的影像和声音信息经过压缩处理后放上网站服务器，让用户一边下载一边观看、收听，而不需要翟畸个压缩文件全部下载到自己机器后才可以观看的技术。

>* 声道：我们经常听说的单声首、立体声，指的就是这里要讲的声道。声道是指声音在录制或播放时在不同空间位置采集或回放的相互独立的音频信号。

常见的声道数有单声道（Mono）、立体声（Stereo）、5.1声道。顾名思义，单声道就是只有一个声道，声音听起来缺乏位置定位，而立体声正好弥补了这一缺点。立体声即双声道，通过在声音录制过程中被分配到两个独立的声道，从而达到了比较好的声音定位效果。而5.1声道则更进一步强化了这种位置感，使人感觉环绕在声音的现场中。5.1声道的输出包括中央声道、前置主左/右声道、后置左/右环绕声道，以及所谓的“0.1”即重低音声道。

>* 语谱仪：用图形来表示语音信号时变谱。
>* 封装格式：就是把视频文件和音频文件打包成一个文件的规范，如avi，rmvb，mp4，flv，mkv。仅仅靠看文件的后缀很难看出具体使用了什么视音频编码标准，总的来说，不同的封装格式之间差距不大，各有优势。有些封装格式支持的视音频编码标准十分广泛，应该算比较优秀的封装格式，比如mkv；而有些封装格式支持的是音频编码标准很少，应该属于比较落后的封装格式，比如RMVB。

音频技术主要包括以: 封装技术，视频压缩编码技术，音频压缩编码技术，流媒体协议技术（网络传输）. 只有avi不支持流媒体（即边下边播），该封装给是通常用于bt下载影视，因此我们下载的电影可以是avi格式mp4，mkv，flv都可用于互联网视频网站，因此从视频网站下载的视频基本是这三个格式ts封装格式用于IPTV和数字电视RMVB是比较老的一种封装格式，用于BT下载影视，因此下载的电影也可是这种格式。

#### 回声

回声路径的时延主要由 __系统时延__ 和 __扬声器到麦克风的传播时间__ 构成的。

>* 系统时延：缓存在音频设备中准备播放和录制的数据按理来说，增加适应滤波器的抽头系数（filter taps）可以处理时延的问题，但是抽头系数的增加就会降低收敛速度，因此在滤波器进行滤波之前，要对时延进行修正，如果时延修正的好，滤波器的系数就不需要设计的很长。

通常采用的时延估计方法为 __互相关检测方法__ ：

>* 互相关检测方法: 该方法的原理就是计算远端信号和近端信号的互相关性，互相关最大所对应的时延就为所估计的时延.

互相关检测方法的缺点：

>* 1.在时延估计之前如果不对信号做任何的处理，该方法对回声路径的改变敏感，因此在估计之前对信号做一些修改(如使用时域包络)，会增加算法的鲁棒性；但是无论是否对信号进行修改，只要是基于相关性的算法，总有一个缺点，那就是：当两个信号存在线性相关性的时候，相关性算法执行效果比较好，但是如果发生非线性扭曲比如频率耦合的时候，相关性算法的鲁棒性就不好了。
>* 2.在时延检测过程中，计算复杂度是一个主要的影响因素 。通常一个系统的时延可达到300ms，在这种情况下，假如采样频率为16kHz并且没有先验知识(priori)的情况下，为了能达到可以检测300ms时延的上限，那么就需要有4800(300*（16000/1000）)个候选点待处理;此外互相关需要长序才能有比较好的执行结果，无疑，这又增加了算法的复杂度。有许多中方式降低算法的复杂度，比如通过下采样或者使用稀疏搜索策略，然而采用这种方式就需要我们在算法复杂度和精确度之间做一个平衡（trade-off），这样的话，如何在不降低性能的情况下来降低精确度是我们要面临的问题在AES和AEC中，处理的是频域数据并且处理的基本单位是采样点块，这就意味着时延估计的分辨率是采样点块而不是采样点，因此零时延误差意味着远端信号和近端信号的对齐是块的对齐(block-wise)，这就说明，采样点级的对齐是无法保证的。

#### aec

两种声学回声：直接回声，间接回声.

>* 直接回声：扬声器出来的声音没有经过任何中间过程而直接进入麦克风，特点：回声的延迟时间非常短，相对来说对通话质量的影响就不太明显
>* 间接回声：扬声器出来的声音经过很多的中间处理过程，如在墙壁等物体上经过多次的反射，最终全部叠加到麦克风之中，形成了一个回声的集合，同理可知，这种回声的延迟时间比较长，通常都在50到300毫秒左右，进而会引起滤波器的阶数变高，对通话质量的影响也变得尤为明显。

在实际的回声消除过程中，间接回声才是真正的关键与难点。间接回声的一种场景：

>* 当用户使用手机或者免提功能时，采集到的声音信号经过很多次的空间反射最终叠加到话筒中去这种声学回声比电学回声的复杂度要高得多，主要是因为从听筒出来的声音有很多的传输和反射路径，从而能够将时延和幅度不同的语音信号全部传送到话筒中去。由于现在手机尺寸越来越小，信号从听筒出来到话筒直接耦合，还有通过用户的反射都比较大，甚至还有的存在空间反射。这一过程产生的回声在移动通信系统中，经过若干次的处理和编码解码工作，能够产生很大的时延。如果不消除这种回声，当与用户通话的时候便会听到自己的回声。一般来说，各种回声消除设备距离回声源越近，那么回声消除的效果就越明显。

IP语音回声的特点：

>* 1.回声源复杂。
>* 2.回声路径的时延大。
>* 3.回声路径的时延抖动大。[因为传输和反射的路径比较多]

回声消除原理：模拟回声，然后扣除回声.

>* 具体来讲：首先预估回声路径中的相关特征参数，接着形成一个模拟的回声路径，然后再计算出模拟的回声信号，最后再从接收的信号当中扣除该信号，从而实现回声消除。途中x(n)表示近端信号，y(n)表示远端信号，r(n)表示真实的回声路径形成的回声，D端叠加有不期望的回声。回声消除器将接收到的远端信号当作参考信号，然后依据自适应滤波器产生的回声估计值r1(n)，将r1(n)从近端信号中扣除，这样便计算出近端传出去的信号u(n)=x(n)+r(n)-r1(n),理想情况下，回声经过回声消除器处理后，残留的回声误差几乎为0，从而能将回声真正的消除。

![aec](https://github.com/lmrshare/lmrshare.github.io/blob/master/images/posts/wiznote/aec.png)

自适应回声消除器有两个关键的组成部分：

>* 1.自适应滤波算法
>* 2.双端语音检测算法(三种语音状态：近端讲话，远端讲话，双端讲话。)

经典自适应回声消除算法：

>* LMS（最小均方算法）
>* NLMS
>* Howells-Applebaum
>* 最小二乘算法(LS)
>* 递推最小二乘算法(RLS)
>* 采样矩阵求逆算法(SMI)
>* 正交投影算法

导致因特网语音延迟的来源主要有三种：

>* 语音压缩延迟(比较主要的时延)
>* 系统处理延迟
>* 分组传输延迟

系统处理延迟：对语音包的封装时延及其在缓冲区的缓冲时延，简单的理解就是编码后到发包前的时间段。
判别aec算法好坏的标准：收敛速度快，计算复杂度低，稳定性好，失调误差小

#### 通信

从研究信息的传输角度来讲，通信的有效性和可靠性是主要的矛盾，有效性体现传输的速度问题，可靠性体现传输的质量问题.

一些概念:

>* 1.码元速率，传码率：单位时间传送 __码元__ 的数目
>* 2.传信率，比特率：单位时间内传递的平均信息量或比特数
>* 3.频带利用率:单位带宽的传输速率(频带利用率 = 传码率/带宽 or 频带利用率 = 比特率/带宽)

通信系统的可靠性可用差错率来衡量，差错率常用误码率和误信率来表示:

>* 1.误码率：码元在传输过程中，被传错的概率(p = 错误码元数/传输总码数)
>* 2.误信率：错误接收比特率的概率(p = 错误比特数/传输总比特数)

#### tools

NDK:

>* NDK（Native Developmengt Kit）是一个需要在linux环境下才能编译生成其他平台可运行的二进制文件的开发工具集. 通俗理解：a).跑在linux上的生成windows可执行文件的工具集；b)是一个编译工具.

cygwin:

>* cygwin是一个在windows平台上运行的unix模拟环境,它对于学习unix/linux操作环境，或者从unix到windows的应用程序移植，非常有用。通过它，你就可以在不安装linux的情况下使用NDK来编译C、C++代码，也就是说如果在linux平台上就可以直接只用NDK

JNI（Java Native interface）提供api

>* 实现java和其他语言的通信，也就是说JNI就是一个adapter，第三方应用是通过JNI来调用自己的C动态库

JNI与NDK

>* a. JNI是java语言提供的java与c/c++互相沟通的机制，它是java的特性，与android无关，在PC上开发java的应用，如果运行在windows平台上使用JNI是经常的，比如读写windos的注册表
>* b. NDK是google为android开发者推出的，帮助Android开发者通过C/C++本地语言编写应用的开发包；NDK是一些列工具的集合（八廓交叉编译工具），帮助android开发者开发C/C++动态库，并能自动将so和java应用打包成apk
>* c. 我的理解：如果有单独的一些工具如交叉编译器，把so和android应用打包成apk的工具的话，可以不使用NDK，因为JNI机制就可以生成so，而NDK可以理解成：为方便android第三方应用调用本地库（C/C++库）的工具

Android NDK开发有以下几个步骤：

>* 1. JNI接口设计
>* 2. 使用C/C++实现本地方法
>* 3. 生成动态链接库
>* 4. 将动态链接库复制到Java工程，运行Java程序

.mk:

>* Android.mk文件：指定我们要编译的so文件所包涵的内容.
>* Application.mk文件：描述应用程序中所需要的原生模块（静态库与动态库）

so文件包含的都是一些模块，可以是静态库，也可以是动态链接库，只有动态链接库被安装进三方应用后，静态库才能被动态链接库使用.

Android:

>* 1.MiniMum Required SDK:最低支持的android api版本，低于这个版本的android手机不能安装应用
>* 2.Target SDK：应用最高支持android api版本
>* 3.compile with： 哪个版本的android SDK编译你的工程，也就是最适合的的，最原生支持你的应用的android 版本

简言之：1：最小；2：最大；3：最适合.

Android应用程序由4个模块构造而成：Activity，Intent，ContentProvider，service；并不是每个应用程序都得有这四个部分。

>* Activity: 代表一个用户所看到的屏幕
>* Intent:实现Activity与Activity的切换
>* content provider：用于不同应用之间共享数据
>* service:后台服务（比如用于播放音乐的进程就可以用service实现）

注意：任何一个应用程序都必须在AndroidManfest.xml文件中声明使用到的这些模块。

物理音量键修改的是 __铃声和通知音音量__

1.输出音量由`audioManager`控制，由`streamType`决定

`streamType` 各个类型的含义：

>* STREAM_VOICE_CALL -----------通话音量（语音通话音量）
>* STREAM_SYSTEM ------------------系统音量（蓝牙音量）
>* STREAM_RING -----------------------铃声音量（铃声和通知音音量）
>* STREAM_MUSIC ----------------------音乐音量（媒体音量，音乐，视频，游戏等的音量）
>* STREAM_ALARM ----------------------提示音音量（闹钟音量）

注意:

>* 括号中的内容是我根据红米手机猜测的，有待验证.
>* 不同的mode（模式，即audioManager的mode参数）下streamtype有不同的表现类型.
>* 不同的streamtype的最大音量时不同的，且最大音量与mode无关.

### 3. Idea

>* 1. 递归最小二乘RLS能否做点事
>* 3. 笔记中的点子
>* 4. 自相关在周期检测中的思想能否做点事
>* 5. 考虑overlap-save算法，思考是否能构造相似的重叠数据以避免混叠
>* 6. multi scale 下用不同方法重建
>* 7. 反卷积网络
>* 8. 加方差
>* 9. 深度稀疏字典即学习个稀疏变换出来，基于压缩传感
>* 10. a pair of face，学习逆函数出来，可以跟压缩传感没关系
>* 11. 在动态成像中，如果采样率一直下降，时间分辨率会增加，但是空间分辨率下降，所以考虑在采样过程中插入以及全采的数据，并且一帧低频，一帧高频


<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/) 

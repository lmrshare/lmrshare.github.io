---
layout: post
title: "CNN及其相关应用"
date: 2018-05-22
description: "Rearch"
tag: Rearch
---


### 目录


* [CNN在人脸对齐上的应用](#cnn-face-alignment)
* [待查](#will-search)


### Backpropagation



### 1. 全连接层(fully connected layers, FC)

`FC`在整个卷积神经网络中起到分类器的作用。

在实际使用中，全连接层可以由卷积操作实现：

    1. 对前层是全连接的全连接层可以转化为卷积和为1x1的卷积
    2. 而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽
(注：这个我还理解不上去)

`FC`层参数存在冗余(占整个网络的80%), 因此近期一些性能优异 的网络模型如ResNet和GoogLeNet等均用全局平均池化(global average pooling, GAP)取代`FC`来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。具体的案例如下：

[冠军之道](https://zhuanlan.zhihu.com/p/23176872)
[project](http://210.28.132.67/weixs/project/APA/APA.html)

如上描述，FC是越来越不被看好的。不过[近期研究]()发现：FC可在模型表示能力迁移过程中充当"防火墙"的作用。具体来说：

    可以在source domain和target domain差异较大的时候保证模型表示能力的迁移，也就是说冗余的参数并不是一无是处。

有关卷积操作“实现”全连接层的一个知识点：

以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：

`filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096`

经过此卷积操作后可得输出为1x1x4096。

如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。

[conv+relu+pooling](http://cs231n.github.io/convolutional-networks/)

### 2. notes of [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)

#### 2.2 part2

stride--就是我以前做的sliding step
padding--边缘处理技巧，常规的有zero padding，我还做过wrapped around

---
ReLU (Rectified Linear Units):

    1. 引入nonlinearity[在线性层之后]
    2. 是一种nonlinear layer，过去常用的有tanh、sigmoid
    3. 相比tanh、sigmoid，ReLU的特点是计算速度快和减轻Vanishing gradient problem
    4. 公式为`f(x) = max(0, x)`, 可以看出ReLU层可以把negative actibations改成0
    5. ReLU可以在不影响卷积层receptive fields的情况下增加模型的nonlinear性质

总结起来：ReLU就是增加model的nonlinear性质

ref:
[hinton paper about ReLU](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)

---
Pooling Layers:

    1. 也称作下采样层, 下采样层中最流行的有`maxpooling`

pos: Pooling Layers的第一段
我感觉maxpooling有点结合等间隔采样和压缩传感的意味, 或者说maxpooling是考虑了数据本身特性的等间隔采样, 这一层的主要作用是：

    1. control overfitting
    2. reduce the amount of parameters or weights

---
Dropout Layers

这一层主要是防止过拟合，有一句比较关键的话是：

    The network shoudl be able to provide the eight classsification or output for a specific example even if some of the activations are dropped out.

ref:
[paper of hinton](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

---
Network in Network Layers

ref:
[paper by min lin](https://arxiv.org/pdf/1312.4400v3.pdf)

---
classfication、localization、detection、segementation...

这篇文章(part 2)后面的内容主要就是讲应用、迁移学习、数据增强。由于涉及的论文较多，我就不一一列出了，后续要看的时候，可以从这篇文章找，文章为：

[A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)

#### 2.3 [part3](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)

搜集了deep learning里比较重要的一些论文

##### [AlexNet (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (cited: 6184)

几个关键点：

    1. 在ImageNet数据上训练了网络结构，具体来讲：用了150,000,00张标注图像, 其中图像来自于超过220,00个类
    2. 利用了比tanh function更快速的ReLU非线性单元，大大缩减了训练事件
    3. 利用了data augmentation techniques来处理图像的translations、horizontal reflection以及patch extraction
    4. 利用dropout layers来对抗overfitting问题
    5. 使用了batch stochastic gradient descent技术进行驯良
    6. 在两个`GTX 580 GPU`上训练了5、6天

为什么重要?

CNNs里的Alex非常具有代表性，因为这是有史以来在ImageNet dataset上表现最好的一个模型, 里面的技术，如: `data augmentation`、 `dropout`现在仍然被使用，这篇
论文阐述了CNNs的优势.

___ref___

- [1. Gradient-Based Learning Applied To Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

###### [ZF Net(2013)](https://arxiv.org/pdf/1311.2901v3.pdf)

自AlexNet之后就有大量的CNNs模型提交到ILSVRC 2013, 其中[Matthew Zeiler](http://www.matthewzeiler.com/research/)和[Rob Fergus](https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php)赢得了比赛, ZF Net的错误率为11.2%. 该模型的体系结构相对于AlexNet的结构更加精细化，同时提出了几个关于改善性能的关键点. 此外，这篇论文的亮点是: 作者花了大量的篇幅来解释ConvNet的动机，并展示如何可视化filters和weights。

这篇论文开篇讨论了"CNNs之所以可以重新吸引人们的兴趣是因为大训练集和GPU的计算性能"。作者也提到：由于缺乏对神经网络内部机制的深入理解，开发更好的模型往往就变成了实验工程(trial and error)，现在神经网络的内部机制仍然是一个黑盒。这篇论文的主要贡献是对AlexNet模型的细节进行轻微调整、对feature map进行可视化。

总结:

>* ___1.___ ZF Net和AlexNet的网络结构仅有细小的差别
>* ___2.___ AlexNet的训练集是15,000,000个图片，而ZF Net仅用了13,000,00个图片
>* ___3.___ ZF Net的第一层滤波尺度不是11x11而是7x7，理由是：第一个卷积层使用小尺度滤波可以跟好的保持原始图像的像素级信息，此外stride用的是2
>* ___4.___ 仍然使用ReLUs作为激励函数来引入nonlinearity，使用cross-entropy作为error function。利用stochastic gradient descent分batch进行训练
>* ___5.___ 在GTX 580 GPU上训练了两个星期
>* ___6.___ 提出了Deconvolutional Network的可视化技术来检验各layer的feature map以及与input的关系。之所以叫做deconvnet是因为该技术将feature map与原始输入图像的pixel进行了映射

DeConvNet

* __Idea:__ 在CNN的每一个layer都附加一个deconvnet使得任意一层的feature map都可以回溯到input pixel，进而完成了映射。

* __实现细节:__ 这部分看的不是很明白，后续看原文(undone)

总结:

ZF Net对CNNs提供了很多直觉性的解释，同时提供多了多种改善性能的方案。可视化策略不仅解释了CNNs的内部工作原理，也对改善神经网络结构给出了比较深刻的解释。

### [VGG Net(2014)](https://arxiv.org/pdf/1409.1556v6.pdf)

19层CNN并且严格限制了滤波的大小为3x3，stride为1，在CNN的后面接了2x2的maxpooling，其中stride为2。VGG Net的特点是__简单__、__深__。

总结：

>* 3x3滤波的使用与AlexNet的11x11和ZF Net的7x7有很大不同。作者的理由是：两个3x3的卷积层与一个5x5的卷积层的表现类似，这就意味着可以在保持小滤波器的尺寸优势的前提下来模拟大滤波器。其中之一的好处就是可以减少参数的个数，此外，利用两个卷积层意味着可以增加两个ReLU
，进而增强模型的非线性。
>* 3个连续的卷积层可以模拟一个7x7。
>* 值得注意的是：在每个maxpooling层之后滤波的数量会增加。这个操作吻合"收缩空间维度、增加深度"
>* 除了在classification和localization任务中表现良好外，作者把一种localization任务当作regression来处理，详细见[paper](https://arxiv.org/pdf/1409.1556v6.pdf)
>* 使用Caffe toolbox
>* 在训练过程中使用了scale jittering作为数据增强技术
>* 在卷积层之后使用ReLU层来增强模型的非线性，并利用batch gradient descent进行训练
>* 在Nvidia Titan Black GPUs上训练了三个星期

#### 为什么重要？

VGG Net 强化了这样一个概念：“卷积神经网络的网络结构应该是由多个层构成的深度网路结构，这样才可以使视觉数据的层次化表示更好的工作”。进而形成这样的一个经验：keep it deep，keep it simple。

### [GooGleNet 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)

简单来说GoogleNet更好的遵守了"Keep it deep, Keep it simple"。GoogleNet使用了22层CNN，其设计原则也颇为简单：堆叠卷积层和pooling层，同时作者也着重强调并处理了机器学习中的著名问题：“复杂模型下的计算复杂度问题以及overfitting问题”。
GoogleNet 引入了Inception module这一概念：

Inception module

GoogleNet的网络结构不是一路串行到底的，这中间会出现一些并行计算单元。通俗来讲：在串行计算的过程中会出现几个分支，然后把分支的计算结果进行concat后再串行。这里的由并行分支构成的单元就称作Inception module，其结构如图所示：

<div align="center">
	<img src="/images/posts/dl-ref-notes/inception_module.png" height="300" width="400">
</div>

在传统的串行网络结构中，在每一层的后面只能接一种操作，比如：pooling op、conv op，而inception module允许并行执行所有的这些操作。但这样做会导致训练没办法工作，主要原因就是输入过多，为了降低输入的深度，GoogleNet在进行3x3或5x5的卷积操作之前会先通过1x1的卷积操作
对depth进行降维，这个操作可被认为是一种特征池化。类似第，GoogleNet通过正常的maxpooling层来降低width和height的维度。关于1x1 conv的详细描述可参考[one by one Convolution](https://iamaaditya.github.io/2016/03/one-by-one-convolution/)。另外有一点需要注意：这些1x1conv后面会跟着ReLU units。现在的疑问是：网络套网络的设计为什么工作呢？答案简单来讲就是：模块中的几个支线可以抽取不同层次的特征(比如：小尺寸滤波系数的卷积操作可以抽取精细化的特征而大尺寸滤波系数的卷积操作可以抽取粒度较粗的特征)，而特征工程又是机器学习领域里很重要的环节。这是一种有失严谨的解释，详细描述看原文吧(不过我认为原文也并没有给出什么严格的数学证明，但肯定比我这样说要convince)。

>* __1.__ 整个网络又9个Inception module，共计100个layer
>* __2.__ 没有使用全连接层，替代它的是平均池化层实现了7x7x1024到1x1x1024的映射
>* __3.__ 在预测的时候，会传输给网络多个同样的图片，然后把这些softmax probabilities平均后作为最终的输出解
>* __4.__ 利用了R-CNN中的一些理念来实现detection model(啥意思，哈哈)
>* __5.__ 在一些"high-end GPUs"上训练了一周

GoogleNet是第一个提出Inception module的网络，也就是说从今天起我CNN同学也不再仅仅是sequentially了哟，总之GooleNet可以深入看下。

### [Microsoft ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)

这个网络是152层的，在ILSVRC 2015上的错误率仅为3.6%，关于ConvNets在图像上的经验可以参考[Andrej Karpathy](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)。在ResNet里有一个Residual Block的概念，Residual Block是
基于这样的考虑：在原始信息基础之上附加一些特征，这样联合了原信息和新特征的输入会更容易进行优化。具体来说，你可以让input x通过一系列conv-relu-conv序列进而得到F(x)，然后将其附加到原input x上得到：H(x) = F(x) + x。此外，Residual Block之所以work的可能原因就是：
在反向传播的时候由于有"+"这个操作，所以梯度更容易贯穿graph。

总结：

>* 152层
>* 值得注意的是：在开始的两个layer之后，spatial size会从224x224变成56x56
>* 作者提到：不假思索的粗暴加层会导致train error和test error的提高
>* 作者尝试了1202层的网络，但结果导致测试精度的下降，猜测可能是overfitting导致的
>* 在8个GPU机器上跑了2~3个星期

ResNet跑了3.6%的error这点很吸引人，由于residual learning的使用ResNet目前是最好的CNN。

<div align="center">
	<img src="/images/posts/dl-ref-notes/residual_block.png" height="300" width="400">
</div>

-[[1] 中文帖](https://www.jianshu.com/p/6908be0c5389)

### [R-CNN(2013)](https://arxiv.org/pdf/1311.2524v5.pdf)、[Fast R-CNN(2015)](https://arxiv.org/pdf/1504.08083.pdf)、[Faster R-CNN(2015)](https://arxiv.org/pdf/1506.01497v3.pdf)

R-CNN是解决object detection任务的，引用率挺高的。R-CNN在做检测任务的时候通常分为成两个步骤：感兴趣区域提取(region proposal)、分类(classification)。对于第一步，RCNN使用了[Selective Search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf)，这个算法可以生成2000个最有可能包含object的区域。获取到这些region后，将其规范到图像尺寸，然后将这些图像作为input喂给训练好的CNN(如AlexNet)，这样就会为每个region image提取feature vector。最后利用linear SVMs(事先为每个class训练好的一个个SVM)对这些feature vector进行分类。补充一点，处于精细化的考虑，还会将这些vector传递给bounding box regressor来获得更精确的box。

#### Fast R-CNN

原R-CNN主要存在速度问题：在训练阶段R-CNN的计算量很大，非常慢，每副图像花费大概53秒。针对这个问题，Fast R-CNN通过共享不同感兴趣区域之间的卷积计算、对感兴趣区域进行排序来进行优化。在Fast R-CNN中，图像首先通过ConvNet，然后从ConvNet的最终的feature map中获得
感兴趣区域的特征，最后，把我们的全连接层作为我们回归或者分类的开始，详细描述见[论文](https://arxiv.org/pdf/1504.08083.pdf)

#### Faster R-CNN

Faster R-CNN主要解决R-CNN和Faster R-CNN略显复杂的训练流程。作者在卷积层的后面插入了一个区域建议网络(region proposal network-RPN)。这个网络可以根据最后的convolutional feature map来生成推荐区域或者称作感兴趣区域，之后再执行与R-CNN同样的训练流程(ROI pooling、FC、开始分类或开始回归)。我的理解就是把之前的Selective Search用conv layers+RPN给换掉。论文亮点：可以确定一个图像中的具体是一个东西，和物体的确切位置。目前Faster R-CNN已经成为object detection算法的标准。

### [Generative Adversarial Networks 2014](https://arxiv.org/pdf/1406.2661v1.pdf)

LeCun提到GAN是未来的一个发展趋势。首先介绍一点关于对抗的例子。例如，对于一个训练好的在ImageNet data上工作良好的CNN网络，我们在一副图像上做轻微的改动以便使预测概率误差增大。虽然图像本身在修改前后看起来一致，但预测结果却可能发生改变。站在另外一个角度来讲可以理解成：数据愚弄了模型。[对抗的例子](https://arxiv.org/pdf/1312.6199v4.pdf)吸引了很多研究员(如mengranlin)，成为现在的热点问题。现在我们讨论生成对抗网络(GAN)，考虑两个模型：生成模型(generative model-gm)和辨识模型(discriminative model-dm)。dl的任务就是区分图像是原始数据集中的还是人造的。gm的任务就是生成人造数据以便训练dm，gm和dm这两个角色像是做着零和博弈。形象点描述可以这样理解：gm是犯罪团伙，小伙子们拼命印假钞，与此同时，dm同学就像警察拼命的打击假钞，gm致力于愚弄dm，而dm拼命反抗愚弄。两个模型都会一直改善直至：仿品无法从真品中辨识。

LeCun在Quora中[提到](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning)：dm现在知道数据的内部表示，因为dm得以训练来理解真实图像同人造图像之间的差异，因此dm可以作为CNN中的特征提取器。另外，你可以制造出比较cool的人造图像](http://soumith.ch/eyescream/)。

### [Generating Image Descriptions](https://arxiv.org/pdf/1412.2306v2.pdf)

当CNNs和RNNs结合的时候会发生什么？Fei-Fei Li和Andrej Karpathy两位大牛逼货结合了CNN和RNN为图像的不同区域生成了自然语言描述。一般情况下，我们喂给CNN的数据都是image+比较简单清晰的标签，而在这个任务里，标签是一句话，其中句子的每个segment和图像的一部分区域对应，这叫做weak label。利用这样的训练数据会训练出可以把segment和图像region对应(alignment)起来的深度神经网络，这个网络叫做Alignment Model。此外，另外一个神经网络会将一副图像作为输入，然后生成一句文本描述。接下来分别介绍这两个模型：Alignment 和 Generation。

#### Alignment Model

这个模型的作用就是把视觉和文本数据对应起来，该模型将图像和句子作为输入, 并输出得分来评价匹配的好坏程度. 接下来我们首先考虑图像表示: 第一步，将图像喂给R-CNN来检测出一个个独立的object. R-CNN是在imagenet上训练出来的. 包括原图在内共有20个区域被嵌入在500-d的sapce内，因此每个图像有20个500-d的向量，这样我们就有了图像信息，接下来是获取句子信息的内容了。这篇论文利用了RNN来将words/segements映射到同样的多模空间内，这样image和sentence就都在同样的多模空间内了，这也就意味我们可以通过计算他们的inner-product来对他们的相似性金星度量。

#### Generation Model

对齐模型的作用是构建由图像区域和文本描述子组成的数据集，然后生成模型从这个数据集中进行学习来获得可以为图像生成描述子的能力。

### [Spatial Transformer Networks](https://arxiv.org/pdf/1506.02025.pdf)

在传统的CNN中，如果希望自己训练的模型对图像的尺寸以及旋转具有不变性(在CNN中，处理spatial invariance的模块是maxpooling layer)，那么事先要准备很多具有这样特点的训练样本，而Google的这篇论文通过引入Spatial Transformer module(stm)来解决这个问题, 也就是说该module关心两个问题：

>* 1. pose normalization
>* 2. spatial attention

stm:

>* Localization network: 输入图像，输出放射变换参数（6个）
>* Grid generator: 利用前面的放射变换参数来改变常规的采样网格
>* sampler: 利用构造好的grids进行采样

可见相对于CNN的常规maxpool，stm不再是预定义的。stm是动态的、具有多种行为的. stm可以顺利的插入到CNN中来对feature map进行变换，进而在训练期间帮助minimize cost function.

这篇论文主要对输入图像进行放射变换来使模型对translation, scale, and rotation具有不变性。论文并没有对CNN的体系结构做改动。这是关于stm的一个[讨论](https://www.quora.com/How-do-spatial-transformer-networks-work).

以上是对Convnet的初步讨论，更细节的东西可以参考Stanford CS 231n lecture videos . 接下来我会通过几个应用进一步讲解.

### <a name="cnn-face-alignment"></a>CNN在人脸对齐上的应用

先对Facial Landmark Detection by Deep Multi-task Learning(2014年的文章)论文做个笔记

#### Facial Landmark Detection by Deep Multi-task Learning(TCDCN)

TCDCN将人脸标记点检测任务与其他相关任务(related tasks)联合起来一起优化. TCDCN构造了一个task-constrained loss function来使related task的error得以反响传播来改善标记点检测任务.
为了调和不同难度、不同收敛速率的任务, 专门设计了任务级的(task-wise)停止规则来加速收敛。

TCDCN从raw pixels中学习feature representation而不是预定义的HOG 人脸描述子. 

知识点：

>* regularization term的作用就是惩罚weights的complexity
>* 通常的MTL最大化所有任务的性能, 而TCDCN希望在相关任务的辅助下最大化主要任务的性能, 即人脸标记点检测.

SGD对于单一任务的学习是有效的, 但是对于多任务的学习却没那么容易, 原因在于: 不同任务具有不同的收敛速率. 现有的解决这个问题的方案为: 利用任务之间的相关性, 例如: 学习一个所有任务权重的协方差矩阵. 然而, 这个方法的局限在于需要要求所有任务的loss function是相同的, 可见对于具有不同loss function的多任务系统显然是不适用的. 此外, 当weight向量的维度很高时, 计算协方差矩阵的代价是很高的.

TCDCN提出early stop规则来及时停止辅助任务的学习以防止其对training set过拟合. 这里有一点要注意early stop规则的regulalization与惩罚权重的regulalizaion是不同的. early stop规则综合考虑了任务在training error的下降趋势以及在validation set上的泛化能力。也就是希望达到这样的目的：如果在training set上的error下降很快则倾向于继续训练. 

网络结构: 输入是40x40的灰度人脸图像, 4个卷积层, 3个池化层, 1个全连接层. 每个卷积层会输出多个feature map. 卷积层中的activation function是absolute tangent function. 在池化阶段, 将max-pooling应用在feature map的non-overlap regions. 最后全连接层输出主要任务、辅助任务共享使用的feature vector. 结构如下图:

<div align="center">
	<img src="/images/posts/cnn-blog/structure_spcification_for_TCDCN.png" height="600" width="800">
</div>

TCDCN与cascade CNN的对比: ****更好的检测精度、更低的计算消耗. 

总结：这篇论文并不没有利用cascade, 模型小, 实时性较好, 可以尝试将其整合到我的工作中, 试试效果.


position_c:找代码以及看文中提到的cascade CNN.

### <a name="will-search"></a>待查

>* TCDCN这篇文章的Related work对人脸方法的总结可以应用到我对face alignemnt的总结里
>* Hinge loss
>* 怎样将论文中的辅助任务整合到普通模型中

___ref___

- [1. 香港中文大学多媒体实验室](http://mmlab.ie.cuhk.edu.hk/publications.html)
- [2. Face Alignment by Coarse-to_fine Shape Searching](http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2015_alignment.pdf)
- [3. Face Alignment by Coarse-to_fine Shape Searching(Code)](http://mmlab.ie.cuhk.edu.hk/projects/CFSS.html)
- [4. Facial Landmark Detection by Deep Multi-task Learning](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf)
- [5. Deep convolutional network cascade for facial point detection 2013](http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf)

<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/) 

---
layout: post
title: "DL相关代码中的笔记(1)"
date: 2018-05-22
description: "Rearch"
tag: Rearch
---

### 1. 全连接层(fully connected layers, FC)

`FC`在整个卷积神经网络中起到分类器的作用。

在实际使用中，全连接层可以由卷积操作实现：

    1. 对前层是全连接的全连接层可以转化为卷积和为1x1的卷积
    2. 而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽
(注：这个我还理解不上去)

`FC`层参数存在冗余(占整个网络的80%), 因此近期一些性能优异 的网络模型如ResNet和GoogLeNet等均用全局平均池化(global average pooling, GAP)取代`FC`来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。具体的案例如下：

[冠军之道](https://zhuanlan.zhihu.com/p/23176872)
[project](http://210.28.132.67/weixs/project/APA/APA.html)

如上描述，FC是越来越不被看好的。不过[近期研究]()发现：FC可在模型表示能力迁移过程中充当"防火墙"的作用。具体来说：

    可以在source domain和target domain差异较大的时候保证模型表示能力的迁移，也就是说冗余的参数并不是一无是处。

有关卷积操作“实现”全连接层的一个知识点：

以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：

`filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096`

经过此卷积操作后可得输出为1x1x4096。

如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。

[conv+relu+pooling](http://cs231n.github.io/convolutional-networks/)

### 2. notes of [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)

#### 2.2 part2

stride--就是我以前做的sliding step
padding--边缘处理技巧，常规的有zero padding，我还做过wrapped around

---
ReLU (Rectified Linear Units):

    1. 引入nonlinearity[在线性层之后]
    2. 是一种nonlinear layer，过去常用的有tanh、sigmoid
    3. 相比tanh、sigmoid，ReLU的特点是计算速度快和减轻Vanishing gradient problem
    4. 公式为`f(x) = max(0, x)`, 可以看出ReLU层可以把negative actibations改成0
    5. ReLU可以在不影响卷积层receptive fields的情况下增加模型的nonlinear性质

总结起来：ReLU就是增加model的nonlinear性质

ref:
[hinton paper about ReLU](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)

---
Pooling Layers:

    1. 也称作下采样层, 下采样层中最流行的有`maxpooling`

pos: Pooling Layers的第一段
我感觉maxpooling有点结合等间隔采样和压缩传感的意味, 或者说maxpooling是考虑了数据本身特性的等间隔采样, 这一层的主要作用是：

    1. control overfitting
    2. reduce the amount of parameters or weights

---
Dropout Layers

这一层主要是防止过拟合，有一句比较关键的话是：

    The network shoudl be able to provide the eight classsification or output for a specific example even if some of the activations are dropped out.

ref:
[paper of hinton](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

---
Network in Network Layers

ref:
[paper by min lin](https://arxiv.org/pdf/1312.4400v3.pdf)

---
classfication、localization、detection、segementation...

这篇文章(part 2)后面的内容主要就是讲应用、迁移学习、数据增强。由于涉及的论文较多，我就不一一列出了，后续要看的时候，可以从这篇文章找，文章为：

[A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)

#### 2.3 [part3](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)

搜集了deep learning里比较重要的一些论文

[AlexNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (cited: 6184)

几个关键点：

    1. 在ImageNet数据上训练了网络结构，具体来讲：用了150,000,00张标注图像, 其中图像来自于超过220,00个类
    2. 利用了比tanh function更快速的ReLU非线性单元，大大缩减了训练事件
    3. 利用了data augmentation techniques来处理图像的translations、horizontal reflection以及patch extraction
    4. 利用dropout layers来对抗overfitting问题
    5. 使用了batch stochastic gradient descent技术进行驯良
    6. 在两个`GTX 580 GPU`上训练了5、6天

为什么重要?

position: why It's Important


<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/) 

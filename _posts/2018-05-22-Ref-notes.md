---
layout: post
title: "DL相关代码中的笔记(1)"
date: 2018-05-22
description: "Rearch"
tag: Rearch
---

### 1. 全连接层(fully connected layers, FC)

`FC`在整个卷积神经网络中起到分类器的作用。

在实际使用中，全连接层可以由卷积操作实现：

    1. 对前层是全连接的全连接层可以转化为卷积和为1x1的卷积
    2. 而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽
(注：这个我还理解不上去)

`FC`层参数存在冗余(占整个网络的80%), 因此近期一些性能优异 的网络模型如ResNet和GoogLeNet等均用全局平均池化(global average pooling, GAP)取代`FC`来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。具体的案例如下：

[冠军之道](https://zhuanlan.zhihu.com/p/23176872)
[project](http://210.28.132.67/weixs/project/APA/APA.html)

如上描述，FC是越来越不被看好的。不过[近期研究]()发现：FC可在模型表示能力迁移过程中充当"防火墙"的作用。具体来说：

    可以在source domain和target domain差异较大的时候保证模型表示能力的迁移，也就是说冗余的参数并不是一无是处。

有关卷积操作“实现”全连接层的一个知识点：

以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：

`filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096`

经过此卷积操作后可得输出为1x1x4096。

如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。

[conv+relu+pooling](http://cs231n.github.io/convolutional-networks/)

### 2. notes of [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)

#### 2.2 part2

stride--就是我以前做的sliding step
padding--边缘处理技巧，常规的有zero padding，我还做过wrapped around

---
ReLU (Rectified Linear Units):

    1. 引入nonlinearity[在线性层之后]
    2. 是一种nonlinear layer，过去常用的有tanh、sigmoid
    3. 相比tanh、sigmoid，ReLU的特点是计算速度快和减轻Vanishing gradient problem
    4. 公式为`f(x) = max(0, x)`, 可以看出ReLU层可以把negative actibations改成0
    5. ReLU可以在不影响卷积层receptive fields的情况下增加模型的nonlinear性质

总结起来：ReLU就是增加model的nonlinear性质

ref:
[hinton paper about ReLU](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)

---
Pooling Layers:

    1. 也称作下采样层, 下采样层中最流行的有`maxpooling`

pos: Pooling Layers的第一段
我感觉maxpooling有点结合等间隔采样和压缩传感的意味, 或者说maxpooling是考虑了数据本身特性的等间隔采样, 这一层的主要作用是：

    1. control overfitting
    2. reduce the amount of parameters or weights

---
Dropout Layers

这一层主要是防止过拟合，有一句比较关键的话是：

    The network shoudl be able to provide the eight classsification or output for a specific example even if some of the activations are dropped out.

ref:
[paper of hinton](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

---
Network in Network Layers

ref:
[paper by min lin](https://arxiv.org/pdf/1312.4400v3.pdf)

---
classfication、localization、detection、segementation...

这篇文章(part 2)后面的内容主要就是讲应用、迁移学习、数据增强。由于涉及的论文较多，我就不一一列出了，后续要看的时候，可以从这篇文章找，文章为：

[A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)

#### 2.3 [part3](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)

搜集了deep learning里比较重要的一些论文

##### [AlexNet (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (cited: 6184)

几个关键点：

    1. 在ImageNet数据上训练了网络结构，具体来讲：用了150,000,00张标注图像, 其中图像来自于超过220,00个类
    2. 利用了比tanh function更快速的ReLU非线性单元，大大缩减了训练事件
    3. 利用了data augmentation techniques来处理图像的translations、horizontal reflection以及patch extraction
    4. 利用dropout layers来对抗overfitting问题
    5. 使用了batch stochastic gradient descent技术进行驯良
    6. 在两个`GTX 580 GPU`上训练了5、6天

为什么重要?

CNNs里的Alex非常具有代表性，因为这是有史以来在ImageNet dataset上表现最好的一个模型, 里面的技术，如: `data augmentation`、 `dropout`现在仍然被使用，这篇
论文阐述了CNNs的优势.

___ref___

- [1. Gradient-Based Learning Applied To Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

###### [ZF Net(2013)](https://arxiv.org/pdf/1311.2901v3.pdf)

自AlexNet之后就有大量的CNNs模型提交到ILSVRC 2013, 其中[Matthew Zeiler](http://www.matthewzeiler.com/research/)和[Rob Fergus](https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php)赢得了比赛, ZF Net的错误率为11.2%. 该模型的体系结构相对于AlexNet的结构更加精细化，同时提出了几个关于改善性能的关键点. 此外，这篇论文的亮点是: 作者花了大量的篇幅来解释ConvNet的动机，并展示如何可视化filters和weights。

这篇论文开篇讨论了"CNNs之所以可以重新吸引人们的兴趣是因为大训练集和GPU的计算性能"。作者也提到：由于缺乏对神经网络内部机制的深入理解，开发更好的模型往往就变成了实验工程(trial and error)，现在神经网络的内部机制仍然是一个黑盒。这篇论文的主要贡献是对AlexNet模型的细节进行轻微调整、对feature map进行可视化。

总结:

>* ___1.___ ZF Net和AlexNet的网络结构仅有细小的差别
>* ___2.___ AlexNet的训练集是15,000,000个图片，而ZF Net仅用了13,000,00个图片
>* ___3.___ ZF Net的第一层滤波尺度不是11x11而是7x7，理由是：第一个卷积层使用小尺度滤波可以跟好的保持原始图像的像素级信息，此外stride用的是2
>* ___4.___ 仍然使用ReLUs作为激励函数来引入nonlinearity，使用cross-entropy作为error function。利用stochastic gradient descent分batch进行训练
>* ___5.___ 在GTX 580 GPU上训练了两个星期
>* ___6.___ 提出了Deconvolutional Network的可视化技术来检验各layer的feature map以及与input的关系。之所以叫做deconvnet是因为该技术将feature map与原始输入图像的pixel进行了映射

DeConvNet

* __Idea:__ 在CNN的每一个layer都附加一个deconvnet使得任意一层的feature map都可以回溯到input pixel，进而完成了映射。

* __实现细节:__ 这部分看的不是很明白，后续看原文(undone)

总结:

ZF Net对CNNs提供了很多直觉性的解释，同时提供多了多种改善性能的方案。可视化策略不仅解释了CNNs的内部工作原理，也对改善神经网络结构给出了比较深刻的解释。

### [VGG Net(2014)](https://arxiv.org/pdf/1409.1556v6.pdf)

19层CNN并且严格限制了滤波的大小为3x3，stride为1，在CNN的后面接了2x2的maxpooling，其中stride为2。VGG Net的特点是__简单__、__深__。

总结：

>* 3x3滤波的使用与AlexNet的11x11和ZF Net的7x7有很大不同。作者的理由是：两个3x3的卷积层与一个5x5的卷积层的表现类似，这就意味着可以在保持小滤波器的尺寸优势的前提下来模拟大滤波器。其中之一的好处就是可以减少参数的个数，此外，利用两个卷积层意味着可以增加两个ReLU
，进而增强模型的非线性。
>* 3个连续的卷积层可以模拟一个7x7。
>* 值得注意的是：在每个maxpooling层之后滤波的数量会增加。这个操作吻合"收缩空间维度、增加深度"
>* 除了在classification和localization任务中表现良好外，作者把一种localization任务当作regression来处理，详细见[paper](https://arxiv.org/pdf/1409.1556v6.pdf)
>* 使用Caffe toolbox
>* 在训练过程中使用了scale jittering作为数据增强技术
>* 在卷积层之后使用ReLU层来增强模型的非线性，并利用batch gradient descent进行训练
>* 在Nvidia Titan Black GPUs上训练了三个星期

#### 为什么重要？

VGG Net 强化了这样一个概念：“卷积神经网络的网络结构应该是由多个层构成的深度网路结构，这样才可以使视觉数据的层次化表示更好的工作”。进而形成这样的一个经验：keep it deep，keep it simple。

### [GooGleNet 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)

简单来说GoogleNet更好的遵守了"Keep it deep, Keep it simple"。GoogleNet使用了22层CNN，其设计原则也颇为简单：堆叠卷积层和pooling层，同时作者也着重强调并处理了机器学习中的著名问题：“复杂模型下的计算复杂度问题以及overfitting问题”。
GoogleNet 引入了Inception module这一概念：

Inception module

GoogleNet的网络结构不是一路串行到底的，这中间会出现一些并行计算单元。通俗来讲：在串行计算的过程中会出现几个分支，然后把分支的计算结果进行concat后再串行。这里的由并行分支构成的单元就称作Inception module，其结构如图所示：

<div align="center">
	<img src="/images/posts/dl-ref-notes/inception_module.png" height="300" width="400">
</div>

在传统的串行网络结构中，在每一层的后面只能接一种操作，比如：pooling op、conv op，而inception module允许并行执行所有的这些操作。


position_c: ["naive idea that hte authors came up with"](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)


<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/) 

---
layout: post
title: "ADMM-net MRI和CNN SR"
date: 2018-10-01
description: "Rearch"
tag: Research
---

### 目录

* [简介](#abstract)
* [ADMM-net-CS-MRI](#admm-net-mri)
* [GAN-sr](#gan-sr)
* [Reference](#Reference)

### <a name="abstract"></a>简介

&emsp;&emsp;核磁共振成像(Magnetic Resonance Imaging------MRI)问题和超分辨率(Super Resolution------SR)问题比较接近, 即都是inverse problem. 所以本文对比调研了深度学习方法在这两个问题上的应用, 目的在于: 通过对一些具有代表性的方法的探讨, 了解深度学习在inverse problem上的进展.

### <a name="admm-net-mri"></a>ADMM-net-CS-MRI

&emsp;&emsp;Deep ADMM-Net for Compressive Sensing MRI(ADMM-Net-CS-MRI)是16年的一篇文章, 目前为止, 我觉得这篇文章在模型解释方面写的比较好, 作者清楚的给出了
ADMM-Net模型的理论依据, 以及ADMM-Net同传统CS-MRI模型的联系, 不会给人为了用深度学习而用深度学习的感觉, 因此决定介绍这篇文章.

&emsp;&emsp;ADMM-Net的模型仍然是压缩感知核磁共振成像(Compressed Sensing MRI------CS-MRI), 可以认为ADMM-Net是CS-MRI下的一种深度学习求解方法. 首先, 作者给出了
CS-MRI的一种求解方法ADMM-solver; 然后, 将ADMM-solver的迭代展开, 创造性地将迭代过程看作数据流动, 进而构造了Data flow Graph数据结构, Graph中的节点与ADMM-solver
中的每个操作一一对应, Graph中的边则代表"数据由一个节点流向另一个节点", 这样就把ADMM-solver的迭代过程与Graph对应起来了. 最后, 作者通过将Graph中的节点/操作概
括成层(layer), 而将Graph构造成了ADMM-Net深度神经网络结构. 整个过程如图1所示:

<div align="center">
	<img src="/images/posts/mri-sr/overview.png" height="500" width="1000">
</div>

$$图1. ovewview$$

接下来, 文章按照如下方式展开: 1).介绍ADMM-Net的建模过程; 2).求解以及训练; 3). 小结.

#### __ADMM-Net的建模过程__

&emsp;&emsp;在CS框架下, MRI成像问题可以描述为:

$$

arg \min_{x} \frac{1}{2} \left\| Ax-y \right\|_2^2 +  \sum_{l=1}^{L} \lambda_l \left\| D_l x \right\|_1

\tag{1}
$$

$\left\| * \right\|_2^2$是保真度(fidelity), 这一项是必有的. $\left\|*\right\|_1$是稀疏约束, 这一项就是CS所特有的. 基于CS的方法的一个主要研究方向就是寻找$D_l$
使$D_l *x$更稀疏. 这一方向的研究有两类比较具有代表性: 1. 预定义稀疏变换(predefined), 如: DCT、DWT、contourlet; 2. 字典学习(dictionary learning), 如: DLMRI.  

&emsp;&emsp;对于这个问题, 我们求解的时候利用非线性共轭梯度法即可; 而这篇文章的作者利用了ADMM算法进行求解, 简单来说: ADMM就是把问题拆解成几个子问题$(2)$,
然后进行迭代求解直至收敛. 

$$

\begin{cases}
X^{(n)}: x^{(n)} = F^T[P^T P + \sum_{l=1}^{L} \rho_l F D_l^T D_l F^T ]^{-1}[P^T y + \sum_{l=1}^{L} \rho_l F D_l^T (z_l^{(n-1)} - \beta_l^{(n-1)}) ] \\
Z^{(n)}: z_l^{(n)} = S(D_l x^{(n)} + \beta_l^{(n-1)}); \lambda_l / \rho_l \\
M^{(n)}: \beta_l^{(n)} = \beta_l^{(n-1)} + \eta_l(D_l x^{(n)} - z_l^{(n)})
\end{cases}

\tag{2}
$$

这里, $n$代表第$n$次迭代. 作者将迭代过程拉平, 这样就形成了前文描述的Graph, 也就是图2:

<div align="center">
	<img src="/images/posts/mri-sr/dfg.png" height="200" width="800">
</div>

$$图2. Data\ flow\ graph(引自原文)$$

图中的$stage_{n}$和$(2)$所代表的第$n$次迭代是对应的. 接着, 作者将Graph中的节点/operation一般化成层, 这样Graph就成了ADMM-net神经网络结构(如图3). 所谓的一般化就是将公式$(2)$
中涉及的变换、参数全都变成可学习的参数.

<div align="center">
	<img src="/images/posts/mri-sr/admm-net.png" height="300" width="800">
</div>

$$图3. ADMM-net神经网络结构(引自原文)$$

&emsp;&emsp;文章的建模过程我已经理完了, 接下来是求解、训练过程中涉及到的一些细节.

#### __求解以及训练__

&emsp;&emsp;求解部分利用的是常规的Backpropagation, 如图4所示:

<div align="center">
	<img src="/images/posts/mri-sr/bp.png" height="500" width="800">
</div>

$$图4. Backpropagation$$

另外有个值得注意的点, 在训练的时候, 作者使用ADMM-solver来完成初始化的, 也就是用常规的CS-MRI的求解结果作为神经网络方法的初始值, 我其实想知道如果随机初始化
会怎样.

#### __小结__

&emsp;&emsp;个人觉得ADMM-Net建模部分读起来比较顺畅, 相对于直接用神经网络建模的文章要好懂也更加convince, 因为那样的文章读
完后常常会有的疑问就是: 为什么这样建模? 层之间为什么这样安排? 你的模型怎么就工作了呢? 总的来说, 我对这篇文章的理解就是: 模
型仍是CS-MRI, 神经网络是一种求解工具. 此外, 我的一个疑问是: 如果改变初始化模式, 最终的效果会怎样?

### <a name="gan-sr"></a>GAN-sr

&emsp&emsp;这篇文章纹理的边界和细节部分很自然，很符合人的视觉感受.

### <a name="Reference"></a>Reference

- [1. Deep ADMM-Net for Compressive Sensing MRI](https://papers.nips.cc/paper/6406-deep-admm-net-for-compressive-sensing-mri)
- [dl sr paper list](https://zhuanlan.zhihu.com/p/31236637)

<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/)

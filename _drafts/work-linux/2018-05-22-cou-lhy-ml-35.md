---
layout: post
title: <font color="ff0000">Notes about Machine Learning(lhy 35)</font>
date: 2018-05-22
description: "Research"
tag: Research
---

### 目录

* [Regression](#regression)
* [Gradient Descent](#gradient-descent)

### <a name="regression"></a>Regression

___machines learning 的3个步骤:___

+ 找model(function set)
+ Goodness of function(a. 搜集训练数据 b. 定义loss function(how bad the model is) c. goodness of parameters(w, b), namely pick the best function)
+ Gradient Descent

___model:___

+ Linear model
+ non-linear model

___loss function:___

+ $L(f) = L(w, b)$: w, b是f的参数, 通过trainging data来求.

___gradient descent:___

+ 只要loss function是可微分的, 就可以处理
+ $w^i$, then $ \frac{\mathrm{d} L}{\mathrm{d} w}\| w = w^i $, update $w^{i+1} = w^i + \eta \frac{\mathrm{d} L}{\mathrm{d} w}\| w = w^i \$
+ 对于多参数, 求偏微分就好, 没什么不同, 如图1.
+ gd缺陷: local minima, 但对于linear regression不用担心这个问题, 因为其Loss function是单峰的.

<div align="center">
	<img src="/images/drafts/lhy-video/gd1.png" height="300" width="600">
</div>

$$图1. 多参数的GD更新(源于lhy视频)$$

___选择更好的model(model selection):___

<div align="center">
	<img src="/images/drafts/lhy-video/another_model.png" height="300" width="600">
</div>

$$图2. another\ model(源于lhy视频)$$

但不是越复杂的model就越好, 因为可能有over fitting.

___选择更多的数据(较多的数据可以反应不同维度的信息):___

+ 考虑其他维度的信息, 如物种(traing data变多后)

<div align="center">
	<img src="/images/drafts/lhy-video/multi_d.png" height="300" width="600">
</div>

$$图3. 线性模型(源于lhy视频)$$

$\delta(*)$函数的目的就是对于不同的物种, 选择不同的线性参数.

更复杂点的model:

<div align="center">
	<img src="/images/drafts/lhy-video/multi_d2.png" height="300" width="600">
</div>

$$图4. another\ model(源于lhy视频)$$

___Regularization:___

<div align="center">
	<img src="/images/drafts/lhy-video/regularization.png" height="300" width="600">
</div>

$$图5. Regularization(源于lhy视频)$$

+ regularization可以使参数更小, 进而使函数更平滑
+ 为什么喜欢更平滑的函数: 对noise比较不敏感; 但又不能过于平滑, 比如极端平滑的一条直线就没有预测性质了.

___Gradient Descent Demo:___

<div align="center">
	<img src="/images/drafts/lhy-video/gd_code1.png" height="300" width="600">
</div>
<div align="center">
	<img src="/images/drafts/lhy-video/gd_code2.png" height="300" width="600">
</div>
<div align="center">
	<img src="/images/drafts/lhy-video/gd_code3.png" height="300" width="600">
</div>
<div align="center">
	<img src="/images/drafts/lhy-video/gd_code4.png" height="300" width="600">
</div>

$$图6. code(源于lhy视频)$$

让$w$和$b$具有不同的learning rate(因为调参搞不定了)

<div align="center">
	<img src="/images/drafts/lhy-video/gd_code5.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/gd_code6.png" height="300" width="600">
</div>

$$图7. 定制化learning\ rate(源于lhy视频)$$

___error的来源:___

+ estimator: bias(瞄准的位置)和variance(准星)

<div align="center">
	<img src="/images/drafts/lhy-video/estimator1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/estimator2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/estimator3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/estimator4.png" height="300" width="600">
</div>

$$图8. estimator(源于lhy视频)$$

+ 简单的model, variance比较小(因为受到不同data的影响小)

<div align="center">
	<img src="/images/drafts/lhy-video/variance.png" height="300" width="600">
</div>

$$图9. variance(源于lhy视频)$$

+ 复杂的model, bias比较小(因为复杂的model的space比较大)

<div align="center">
	<img src="/images/drafts/lhy-video/bias1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bias2.png" height="300" width="600">
</div>

$$图10. bias(源于lhy视频)$$

___bias vs variance:___

<div align="center">
	<img src="/images/drafts/lhy-video/bvsv.png" height="300" width="600">
</div>

$$图11. bias\ vs variance(源于lhy视频)$$

___如何处理underfitting和overfitting:___

+ underfitting: redesign your model

<div align="center">
	<img src="/images/drafts/lhy-video/wtdw.png" height="300" width="600">
</div>

$$图12. what\ to\ do\ with\ large\ bias(源于lhy视频)$$

+ overfitting: 如果你的模型比较复杂但又没足够的数据的时候(可以考虑人造), 考虑使用regularization(虽然可以使曲线平滑, 但是可能伤害bias)

<div align="center">
	<img src="/images/drafts/lhy-video/wtdwlv.png" height="300" width="600">
</div>

$$图13. what\ to\ do\ with\ large\ variance(源于lhy视频)$$

___Model selection:___

+ 如果不做cross validation: 无法保证public testing set上的performance反应在private testing set的performance 

<div align="center">
	<img src="/images/drafts/lhy-video/nocv.png" height="300" width="600">
</div>

$$图14. 不做cross\ validation(源于lhy视频)$$

+ cross validation: 可以保证public testing set的performance反映出priavate testing set的performance

<div align="center">
	<img src="/images/drafts/lhy-video/cv2.png" height="300" width="600">
</div>

$$图15. cross\ validation(源于lhy视频)$$

之所以要N-fold cross validation是因为不相信一次分割的结果, 因此多分几次去平均.

### <a name="gradient-descent"></a>Gradient Descent

___learning rate:___

+ Turning your learning rate

<div align="center">
	<img src="/images/drafts/lhy-video/turn_param.png" height="300" width="600">
</div>

$$图1. learning\ rate(源于lhy视频)$$

+ 通常情况下, learning rate随着训练越来越小
+ Adagrad: 调整learning rate的方法; 原理: 把二阶导数考虑进去, 分母模拟二阶导数

<div align="center">
	<img src="/images/drafts/lhy-video/lr_decay1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/lr_decay2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/lr_decay3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/lr_decay4.png" height="300" width="600">
</div>

$$图2. 调整learning\ rate(源于lhy视频)$$

+ trick: 为什么梯度越大, learning rate越小, 如何解释?(answer: 反差)

<div align="center">
	<img src="/images/drafts/lhy-video/expla1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/expla2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/expla3.png" height="300" width="600">
</div>

$$图3. 解释adagrad(源于lhy视频)$$

___Stochastic Gradient Descent:___

+ 每次拿一笔data出来, 计算loss(只考虑这笔data)
+ 快

<div align="center">
	<img src="/images/drafts/lhy-video/sgd1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/sgd2.png" height="300" width="600">
</div>

$$图4. SGD(源于lhy视频)$$

___Feature Scaling:___

+ 向着圆心走, 比较有效率 
+ 做法1: 对每一个维度, 减均值扣均差

<div align="center">
	<img src="/images/drafts/lhy-video/fs1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/fs2.png" height="300" width="600">
</div>

$$图4. Feature\ Scaling(源于lhy视频)$$

<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/) 

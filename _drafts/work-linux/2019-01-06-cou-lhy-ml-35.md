---
layout: post
title: <font color="ff0000">Notes about Machine Learning(lhy 35-introduction of dl)[2]</font>
date: 2019-01-06
description: "Research"
tag: Research
---

### 目录

* [Introduction of deep learning](#introduction)
* [Backpropagation](#backpropagation)
* [Keras](#keras)
* [Tips for deep learning](#tfdl)
* [Keras2](#keras2)

### <a name="introduction"></a>Introduction of deep learning

___introduction:___

+ fully connect feedforward network

<div align="center">
	<img src="/images/drafts/lhy-video/fully.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/fully2.png" height="300" width="600">
</div>

$$图1. fully\ connect\ feedforwad\ network(源于lhy视频)$$

+ history about layers

<div align="center">
	<img src="/images/drafts/lhy-video/layers.png" height="300" width="600">
</div>

$$图2. layers(源于lhy视频)$$

+ 运作: matrix operation
+ 写成矩阵运算的好处就是可以用GPU加速

<div align="center">
	<img src="/images/drafts/lhy-video/ope1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/ope2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/ope3.png" height="300" width="600">
</div>

$$图3. 运作方式(源于lhy视频)$$

+ Output layer

<div align="center">
	<img src="/images/drafts/lhy-video/output.png" height="300" width="600">
</div>

$$图4. output(源于lhy视频)$$

+ demo
+ 设计网路结构

<div align="center">
	<img src="/images/drafts/lhy-video/demo.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/demo2.png" height="300" width="600">
</div>

$$图5. demo(源于lhy视频)$$

+ FAQ

<div align="center">
	<img src="/images/drafts/lhy-video/faq.png" height="300" width="600">
</div>

$$图6. faq(源于lhy视频)$$

+ 定义一个function的好坏
+ gradient descent
+ toolkits
+ sources

<div align="center">
	<img src="/images/drafts/lhy-video/totalloss.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/gd.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/toolkits.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/source.png" height="300" width="600">
</div>

$$图7. loss(源于lhy视频)$$

### <a name="backpropagation"></a>Backpropagation

+ 参数太多, 如何有效的处理百万维的向量. 用backpropagation
+ chain rule
+ backpropagation: forward pass和backward pass

<div align="center">
	<img src="/images/drafts/lhy-video/bp1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp4.png" height="300" width="600">
</div>

$$下面两个图计算forward\ pass(源于lhy视频)$$

<div align="center">
	<img src="/images/drafts/lhy-video/bp5.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp6.png" height="300" width="600">
</div>

$$下面八个图计算backward\ pass(源于lhy视频)$$

<div align="center">
	<img src="/images/drafts/lhy-video/bp7.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp8.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp9.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp10.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp11.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp12.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp13.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/bp14.png" height="300" width="600">
</div>

$$Summary(源于lhy视频)$$

<div align="center">
	<img src="/images/drafts/lhy-video/bp15.png" height="300" width="600">
</div>

$$图1. backpropagation(源于lhy视频)$$

### <a name="keras"></a>Keras

+ 数据维度
+ batch
+ batch and epoch
+ 运算时间: batch size为1和10的执行时间是不一样的. 这是为什么呢. gpud的并行计算
+ demo: building a network

<div align="center">
	<img src="/images/drafts/lhy-video/dimension.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch4.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch5.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/batch6.png" height="300" width="600">
</div>

$$图1. keras(源于lhy视频)$$

### <a name="tfdl"></a>Tips for deep learning

+ recipe
+ 要检查traing set和testing set上的结果来确认是否是overfitting

<div align="center">
	<img src="/images/drafts/lhy-video/recipe.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/recipe1.png" height="300" width="600">
</div>

$$图1. recipe(源于lhy视频)$$

+ vanishing gradient problem(比如用sigmoid function, 因为该函数会将输入值衰减). 用ReLU可以解决vanishing gradient problem
+ 现在比较常用的activation function是ReLU(Rectified Linear Unit).
+ ReLU会将网络变成瘦长的网络(线性), 从而不会出现sigmoid funcion递减的问题从而解决vanishing gradient problem
+ maxout: 自动学习actication function. ReLU是Maxout的一个special case.

<div align="center">
	<img src="/images/drafts/lhy-video/vanish.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/vanish1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/vanish2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/vanish3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/vanish4.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/vanish5.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/vanish6.png" height="300" width="600">
</div>

$$图2. vanishing] gradient\ problem,\ ReLU,\ Maxout(源于lhy视频)$$

+ maxout-training: 给定输入后, 在max后, 网络变成线性的, 所以自然可以求倒
+ maxpooing 实际上和maxout是一样的

<div align="center">
	<img src="/images/drafts/lhy-video/mtrain.png" height="300" width="600">
</div>

$$图3. train(源于lhy视频)$$

+ Adagrad
+ RMSProp
+ Momentum: 除了梯度外还考虑原来走的方向. 实际上原来的方向是由以前的梯度构成的. 这个方法保证跳出local minima的可能性
+ Adam: RRMProp + Momentum

<div align="center">
	<img src="/images/drafts/lhy-video/review.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/review1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/review2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/review3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/review4.png" height="300" width="600">
</div>

$$图4. review(源于lhy视频)$$

+ early stopping
+ regularization($L_2, L_1$)
+ dropout: training的时候, 每一次update参数之前都对神经元做采样, 按照一定的比例p丢掉一些神经元, 然后整个网络就变得细长, 然后再train这个细长的网络.
+ 注意: 再每一次update参数之前都要做一次sampling, 因此每一次的网络结构都是不一样的
+ testing的时候不做dropout, 需要在weight前乘以(1-p)
+ 最后一幅图是个demo, 这个简单的网络肯定是work的, 但是对于复杂的网络还会相等吗? 答案是: 如果模型是线性模型那会相等, 但是非线性的话就不会相等, dropout神奇的地方在于即使不相等, 模型也会work. 因此有人提出假设: 如果模型接近线性比如用ReLU或者maxout效果是不是会更好. 答案是肯定的, 比如用ReLU或者maxout构建的网络.用dropout后效果可能会更好.
+ 直觉理由

<div align="center">
	<img src="/images/drafts/lhy-video/es.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es1.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es2.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es3.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es4.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es5.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es6.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es7.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es8.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es9.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es10.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es11.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es12.png" height="300" width="600">
</div>

<div align="center">
	<img src="/images/drafts/lhy-video/es13.png" height="300" width="600">
</div>

$$图5. 改进Tesing\ performance(源于lhy视频)$$

### <a name="keras2"></a>Keras2

+ loss、batch size、网络的深度、数据的正则化、activaton的类型、优化器(adam)的类型、dropout都会影响最终的结果以及性能
+ ML Lecture 9-2- Keras Demo 2.mp4有些调参经验
+ ML Lecture 9-3- Fizz Buzz in Tensorflow (sequel).mp4是一个好玩的demo

<br>

转载请注明：[Mengranlin](https://lmrshare.github.io) » [点击阅读原文](https://lmrshare.github.io/2015/09/iOS9_Note/) 
